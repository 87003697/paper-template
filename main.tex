\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version

\usepackage[review,year=2026,ID=153]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}


\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title
\title{OREO: Generalizing 3D Native Generators with On-Policy Rendering-Editing Optimization} 

% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
\titlerunning{Abbreviated paper title}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\author{First Author\inst{1}\orcidlink{0000-1111-2222-3333} \and
Second Author\inst{2,3}\orcidlink{1111-2222-3333-4444} \and
Third Author\inst{3}\orcidlink{2222--3333-4444-5555}}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{F.~Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.
\institute{Princeton University, Princeton NJ 08544, USA \and
Springer Heidelberg, Tiergartenstr.~17, 69121 Heidelberg, Germany
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}

\maketitle


\begin{abstract}
  Despite the success of 3D native generators, they often struggle to generalize to highly stylized scenarios, resulting in geometric or textual misalignment, due to the scarcity of diverse 3D training data.
  To address this challenge, we introduce \textbf{OREO}, a novel post-training paradigm that compensates for 3D data scarcity by leveraging pre-trained 2D diffusion priors within an on-policy \textit{Render-Edit-Optimize} loop.
  At the core of our framework lies the \textbf{Reinforced Editing Distillation (RED)} algorithm. 
  RED employs an enhanced FlowEdit strategy to edit the rendered views, reinforcing target concepts in a view-preserving manner, thereby yielding geometrically consistent pseudo-ground truths. 
  To optimize the 3D generator, these view-space signals are leveraged via a contrastive distillation loss, with the resulting gradients backpropagated through the entire generation trajectory. 
  To prevent geometric degradation, along with the editing backpropagation, we incorporate a 3D native regularization that propagates structural constraints back to early generation steps.
  Extensive experiments demonstrate that OREO significantly outperforms supervised baselines on complex creative inputs, achieving superior conceptual alignment and geometric integrity, effectively mitigating the limitations of 3D data scarcity.
  \keywords{3D Generation \and On-Policy Distillation \and Image Editing \and Flow Matching \and Differentiable Rendering}
\end{abstract}

% NOTE: Terminology Distinction
% - Semantic: Refers to high-level understanding, knowledge, and alignment (e.g., "semantic priors", "semantic misalignment").
% - Concept: Refers to specific objects, attributes, or content being generated/injected (e.g., "inject target concepts", "complex concepts").

\section{Introduction}
\label{sec:intro}

In recent years, 3D native generators such as Trellis and Hunyuan3D series have demonstrated remarkable capabilities in automating high-quality 3D content creation. 
The current mainstream paradigm relies heavily on pre-training with large-scale 3D datasets; however, due to the prohibitive cost of 3D asset construction, these datasets lag far behind the 2D domain in diversity, severely limiting model generalization in non-canonical scenarios. 
This bottleneck is particularly evident in the critical stage of 3D concept design. 
This creative process inherently requires generating imaginative and highly stylized assets that often deviate significantly from realistic prototypes found in standard datasets.
In such scenarios, pre-trained models often fail to generalize, resulting in distorted structures or severe semantic misalignment due to the absence of corresponding 3D prototypes in the training data.

To address this challenge, our core insight is to distill the rich semantic and general visual knowledge from pre-trained 2D diffusion models into 3D generators, thereby enabling the creation of conceptually aligned assets even in data-scarce domains. 
Based on this philosophy, we propose the \textbf{OREO (On-Policy Rendering Editing Optimization)} framework. 
In this context, we view the 3D generator as a \textit{policy} that dictates the trajectory from noise to 3D assets.
Unlike traditional supervised learning that relies on static datasets (off-policy), OREO establishes an \textbf{on-policy} \textit{Render-Edit-Optimize} loop: the model is optimized directly on the views rendered from its \textit{current} generation trajectory.
This allows the generator to receive immediate feedback on its own artifacts from the 2D teacher, effectively bridging the domain gap through self-correction rather than static imitation.

At the heart of this loop is the \textbf{Reinforced Editing Distillation (RED)} algorithm, which provides high-quality supervision signals to the 3D model through two synergistic steps: ``editing'' and ``distillation''. 
First, we treat the rendered image as a ``state sample'' of the student model under the current policy and utilize a pre-trained 2D editing model as a ``teacher'' to provide immediate corrections. 
To ensure the reliability of the correction signals, we introduce an improved FlowEdit strategy to generate geometrically consistent pseudo-ground truths. 
This strategy injects target concepts into the rendered view while preserving the original viewpoint, thereby providing the 3D model with dense and geometry-aware pixel-level supervision signals. 
Subsequently, to stably distill these signals into the 3D generative model, we further propose a Contrastive Distillation Loss. This loss function achieves low-variance gradient updates by constructing positive and negative sample pairs.

To achieve closed-loop optimization of this cycle, OREO is built upon a BPTT-based architecture. 
We utilize a Differentiable Rollout mechanism to implement end-to-end Backpropagation through the Trajectory. 
First, this allows the editing guidance calculated at the terminal state to be backpropagated to every step of the generation process, enabling fine-grained control over the entire trajectory. 
Second, thanks to Backpropagation through the Trajectory, we are able to introduce more potent 3D Native Regularization. 
Geometric constraints from later steps can influence parameter updates in earlier steps via gradient backpropagation, effectively preventing geometric degradation caused by aggressive editing and significantly improving the conceptual fidelity and multi-view consistency of the generated assets.

In summary, our contributions are primarily threefold:
\begin{enumerate}
    \item We pioneer a novel post-training paradigm for 3D generative models, \textbf{OREO}. 
    Addressing the generalization bottleneck of pre-trained models in highly stylized and non-canonical scenarios, OREO effectively resolves the issue of insufficient training data diversity by constructing a \textit{Render-Edit-Optimize} closed-loop mechanism.
    \item We propose the \textbf{Reinforced Editing Distillation (RED)} algorithm combined with a fully differentiable optimization mechanism. 
    The former utilizes FlowEdit and Contrastive Distillation Loss to provide high-quality geometry-aware guidance, while the latter ensures optimization stability and geometric integrity through BPTT and 3D Native Regularization.
    \item We validate the effectiveness of our method on mainstream 3D generative models such as Trellis. 
    Experimental results show that OREO significantly enhances the model's generalization capability on complex creative inputs, surpassing supervised baselines in both conceptual alignment and geometric fidelity.
\end{enumerate}

\section{Method}
\label{sec:method}

\subsection{Preliminaries}

\subsubsection{On-Policy Distillation for 3D Generation.}
We formalize the post-training of 3D generative models as an On-Policy Distillation problem. Given a student model (3D Generator) parameterized by $\theta$, the goal is to learn the distribution $p_{teacher}$ of a teacher model (2D Editor). Unlike offline distillation which relies on a fixed dataset $D = \{(x, y)\}$, On-Policy Distillation requires the student model to learn from trajectories $x \sim \pi_\theta$ generated by itself. This is achieved by minimizing the following divergence:
\begin{equation}
\mathbb{E}_{x \sim \pi_\theta} [ \mathcal{L}(x, \text{Teacher}(x)) ]
\end{equation}
This paradigm ensures that the model can correct geometric deviations generated during its own inference process in real-time, which is crucial for improving generation quality in highly stylized and non-canonical scenarios.

\subsubsection{3D Native Generation via Flow Matching.}
Our goal is to optimize a 3D generative model $\mathcal{G}_\theta$ that has already been pre-trained on large-scale data. Unlike optimization methods initialized from scratch (e.g., DreamFusion), we leverage the pre-trained model as a strong geometric prior. Such models are typically based on the Flow Matching framework, where the generation process is modeled as an ODE integration from a prior distribution $z_T \sim \mathcal{N}(0, I)$ to the data distribution $z_0$:
\begin{equation}
dz_t = v_\theta(z_t, t) dt
\end{equation}
where $v_\theta$ is the velocity field predicted by the network. Given the current state $z_t$ and velocity $v_\theta$, we can derive an estimate of the final data $z_0$: $\hat{z}_0 = z_t - t \cdot v_\theta(z_t, t)$. Subsequently, a decoder converts $z_0$ into an explicit 3D asset (e.g., Gaussian Splats), which is then projected into 2D views via a differentiable renderer $\mathcal{R}$.

\subsubsection{Instruction-based Image Editing.}
The task of image editing is to transform a source image $x^{src}$ into a target image $x^{tgt}$ that conforms to a text instruction $y$, while preserving the original structure unrelated to the instruction. Formally, we seek a mapping $\mathcal{E}: (x^{src}, y) \to x^{tgt}$. In the OREO framework, this mapping $\mathcal{E}$ acts as a ``teacher'', providing pseudo-ground truths of ``what should be generated'' to the 3D model.

\subsubsection{Why FlowEdit Algorithm?}
In the OREO framework, we employ the \textbf{FlowEdit algorithm} to guide the inference process of the base editing model. Compared to default inference modes (e.g., direct sampling), FlowEdit offers unique advantages:
\begin{enumerate}
    \item \textbf{View Locking}: FlowEdit utilizes the parallelogram principle of flow matching to construct a target trajectory parallel to the source trajectory in the latent space. This mechanism implicitly imposes strong geometric constraints, ensuring that editing operations only modify texture and details (Identity) without altering the object's pose or silhouette (Geometry). In contrast, SDE-based methods tend to introduce random viewpoint shifts, leading to divergence in 3D optimization.
    \item \textbf{Training-free \& Generalization}: FlowEdit does not require fine-tuning on specific datasets and can directly leverage the powerful priors of pre-trained Diffusion/Flow models, perfectly aligning with our goal of handling OOD data.
    \item \textbf{Gradient Stability}: Based on deterministic ODE sampling, FlowEdit provides lower variance gradient estimates compared to stochastic SDE sampling.
\end{enumerate}

\subsection{2D Guidance via Reinforced Editing Distillation}

We utilize a pre-trained Flow Matching model $v_\varphi$ (Qwen-Image-Edit in this work) as the base to execute an improved version of the FlowEdit algorithm. To adapt to the 3D post-training task, we introduce three key improvements to the standard FlowEdit. \textbf{It is worth noting that the effectiveness of RED is built on the premise that the 2D editor can generate high-quality pseudo-ground truths (Pseudo-GT). We verify this in Section 4.1 through quantitative experiments, showing that the improved FlowEdit can significantly enhance conceptual consistency while effectively maintaining geometric integrity, thus qualifying for the role of a ``teacher''.}

\begin{enumerate}
    \item \textbf{Negative Guidance}: We introduce a negative guidance scale $-s$ when calculating the source flow field $v^{src}$. This forces the model to identify and preserve features in the source image that do \textbf{not} match the prompt (usually background or geometry that should not be edited), thereby enhancing the locality of the edit.
    \item \textbf{Dynamic Noise Correction}: Standard FlowEdit assumes fixed noise $\epsilon$. However, as the source flow field changes, fixed noise leads to trajectory deviation. We introduce a dynamic correction term that adjusts $\epsilon_t$ in real-time based on the gradient of the source flow field, ensuring the source trajectory remains anchored to the manifold of the rendered image.
    \item \textbf{Prediction Recording}: We record the clean state prediction at $t=0$ at every step. These intermediate predictions contain rich conceptual gradient information and form the basis for the subsequent contrastive loss.
\end{enumerate}

\subsubsection{Contrastive Distillation Loss.}
Inspired by cutting-edge results in Score Distillation (e.g., VSD, CSD), we propose a \textbf{contrastive distillation loss} calculated in the $x_0$-space (image domain). Our loss function is defined as a weighted sum over a set of sampled time steps $\{t\}$:
\begin{equation}
\mathcal{L}_{\text{RED}} = \sum_{t} \omega(t) \left[ \| x^{src} - \hat{x}^{+}_{t \to 0} \|^2 - \| x^{src} - \hat{x}^{-}_{t \to 0} \|^2 \right]
\end{equation}
where $\hat{x}^{+}_{t \to 0}$ is the ideal target prediction (Conditional Prediction) from the FlowEdit target branch using positive guidance ($+s$), acting as an ``attractor''. $\hat{x}^{-}_{t \to 0}$ is the unconditional prediction from the target branch, acting as a ``repeller''. $\omega(t)$ is a time-step weighting function with adaptive gradient normalization: $\omega(t) = 1 / (\| x^{src} - \hat{x}^{+}_{t \to 0} \|_1 + \epsilon)$.

\subsection{Differentiable Rollout \& Optimization}

To achieve end-to-end On-Policy optimization, we construct a fully differentiable generation pipeline. Specifically, given a conditional input, the model first generates a discrete coarse structure (Dense Structure), which remains frozen during training. Subsequently, the flow matching model performs a \textbf{sparse feature rollout} guided by the coarse structure to generate fine-grained 3D latent features $z_0$.

During training, we use a differentiable ODE solver (e.g., Euler Step) to discretize the flow matching integration process:
\begin{equation}
z_{t_{i-1}} = z_{t_i} - (t_i - t_{i-1}) \cdot v_\theta(z_{t_i}, t_i)
\end{equation}
Crucially, we retain the computation graph at every step. Thus, the final generated $z_0$ is a composite function of all velocity field predictions along the entire trajectory:
\begin{equation}
z_0 = \text{Solver}(z_T, \{v_\theta(\cdot, t_i)\}_{i=N}^1)
\end{equation}
Finally, the decoder maps $z_0$ to an explicit 3D representation, which is projected to 2D images via $\mathcal{R}$. This allows supervision signals from the 2D editor to backpropagate through the renderer, decoder, and along the ODE solver's computation graph via \textbf{Backpropagation through the Trajectory}, enabling end-to-end optimization of the entire generation trajectory.

\subsection{3D Regularization via $z_0$-Prediction}

Relying solely on 2D editing signals for 3D optimization is ill-posed. To prevent geometric degradation, we introduce 3D regularization based on $z_0$ prediction:
\begin{equation}
\mathcal{L}_{z_0} = \mathbb{E}_{t \sim [0,1]} \left[ \frac{\| \hat{z}_{0,\theta}(z_t, t) - \hat{z}_{0,frozen}(z_t, t) \|^2}{t^2 + \epsilon} \right]
\end{equation}
where $\hat{z}_{0}(z_t, t) = z_t - t \cdot v(z_t, t)$ is the $z_0$ estimate derived from the Flow Matching formula. This design ensures \textbf{temporal consistency} and \textbf{geometry awareness}, effectively preserving the geometric topology priors contained in the pre-trained model.

The final total optimization objective consists of the terminal editing distillation loss and the intermediate regularization loss:
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{RED}(z_0) + \lambda \mathcal{L}_{z_0}
\end{equation}


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Dataset.}
To validate the effectiveness of OREO in \textbf{3D concept design} scenarios, we constructed a specialized \textbf{Conceptual Design Dataset}.
\begin{itemize}
    \item \textbf{Training Set}: Contains approximately 2,000 high-quality concept design images collected from the internet, covering sci-fi vehicles, fantasy creatures, stylized characters, and futuristic architecture. These images typically feature exaggerated proportions, unique textures, and non-realistic geometric structures, posing extremely high demands on the generalization capability of 3D generators.
    \item \textbf{Test Set}: Contains 100 unseen, highly imaginative design sketches, used to evaluate the model's ability to translate abstract ideas into 3D entities.
\end{itemize}

\subsubsection{Baselines.}
We compare OREO with the following baseline:
\begin{itemize}
    \item \textbf{Trellis (Zero-shot)}: The original pre-trained Trellis model, serving as the supervised learning benchmark.
\end{itemize}

\subsubsection{Metrics.}
We employ a combination of quantitative and qualitative evaluations:
\begin{itemize}
    \item \textbf{CLIP Similarity}: Measures the conceptual consistency between generated views and the input prompt.
    \item \textbf{DINO Similarity}: Evaluates the visual feature similarity between generated results and reference images.
    \item \textbf{User Study}: Human evaluators are invited to score the geometric quality and texture details of the generated assets.
\end{itemize}

\subsubsection{Implementation Details.}
Our experiments are implemented based on the PyTorch framework.
\begin{itemize}
    \item \textbf{Model Architecture}: We use the pre-trained \textbf{Trellis-Image-Large} as the 3D generator backbone and \textbf{Qwen-Image-Edit} as the 2D editing teacher model. The renderer is Gaussian Splatting, with a rendering resolution of $1024 \times 1024$.
    \item \textbf{Training Settings}: We use the \textbf{SGD optimizer} with a learning rate of $5 \times 10^{-3}$. Training is conducted on a single NVIDIA A800 GPU with a batch size of 1 and gradient accumulation steps of 4. To save memory, we use \textbf{BF16 mixed precision} training. The total number of training epochs is 500.
    \item \textbf{RED Parameters}: The noise mode uses the "Aligned" strategy. Based on preliminary analysis (see Section 4.2), we fix the configuration to \textbf{(Steps: 9|12, Cfg=4)}, meaning editing starts at a noise level of $t=0.75$ with a total of 12 inference steps.
    \item \textbf{Loss Weights}: In all experiments, we only use the \textbf{Contrastive Distillation Loss (CDL)} with a weight of 1.0, without enabling additional MSE or regularization losses.
\end{itemize}

\subsection{Preliminary Analysis: Validating the Editing Teacher}

Before applying FlowEdit to the 3D optimization loop, we first evaluated its competence as a "teacher model" on an independent 2D dataset and determined the optimal editing parameters. The upper bound of 3D optimization depends on the quality of the 2D Pseudo-GT: we need to ensure that the $\Delta x$ produced by the editing process is primarily semantic correction rather than geometric destruction.

We selected 50 rendered images from the test set, executed FlowEdit under different parameter configurations, and calculated CLIP Similarity (conceptual alignment), DINO Similarity (structural consistency), and Silhouette IoU (contour overlap). Additionally, we compared FlowEdit with other mainstream image editing methods. Table \ref{tab:flowedit_ablation} shows the detailed quantitative analysis results.

\begin{table}[tb]
  \caption{Sensitivity analysis of FlowEdit parameters. We report the change in metrics (Diff) and the final Mask IoU under different configurations. The gray background indicates the selected final configuration.}
  \label{tab:flowedit_ablation}
  \centering
  \begin{tabular}{@{}llccc@{}}
    \toprule
    Experiment & Configuration & CLIP Sim Diff$\uparrow$ & DINO Sim Diff$\uparrow$ & Mask IoU$\uparrow$ \\
    \midrule
    \textbf{A. CFG Scale} & Steps: 20|40, Cfg=2 & +0.0029 & +0.0028 & 0.9887 \\
    \textit{(Fixed Steps)} & \textbf{Steps: 20|40, Cfg=4} & \textbf{+0.0034} & \textbf{+0.0089} & \textbf{0.9786} \\
    & Steps: 20|40, Cfg=12 & -0.0098 & +0.0051 & 0.9631 \\
    \midrule
    \textbf{B. Ratio (3:4 vs 2:4)} & Steps: 20|40 (Ratio 0.50) & +0.0034 & +0.0089 & 0.9786 \\
    \textit{(Fixed Cfg=4)} & \textbf{Steps: 30|40 (Ratio 0.75)} & \textbf{+0.0228} & \textbf{+0.0226} & \textbf{0.9319} \\
    \midrule
    \textbf{C. Efficiency} & Steps: 30|40 (Total 40) & +0.0228 & +0.0226 & 0.9319 \\
    \textit{(Fixed Ratio 0.75)} & \textbf{Steps: 9|12 (Total 12)} & \textbf{+0.0132} & \textbf{+0.0156} & \textbf{0.9595} \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Analysis and Conclusion}:
\begin{enumerate}
    \item \textbf{Choice of Guidance Scale}: Experiments show that excessively high CFG (e.g., 12) severely damages image quality, leading to drops in both CLIP and IoU. \textbf{Cfg=4} demonstrated the most robust performance, effectively enhancing conceptual features while maintaining high geometric consistency (IoU > 0.97).
    \item \textbf{Critical Role of Timestep Ratio}: We found that the starting point of the editing interval is more critical than the total number of steps. Comparing \textbf{Steps: 20|40 (Ratio 0.5)} and \textbf{Steps: 30|40 (Ratio 0.75)}, the latter brought nearly a \textbf{7x} improvement in CLIP (+0.0228 vs +0.0034). This indicates that at low noise levels like $t=0.5$, the image structure is solidified, and the model lacks sufficient \textbf{plasticity} to inject new concepts. Therefore, we fix the \textbf{Ratio to 0.75} to ensure sufficient editing freedom.
    \item \textbf{Efficiency-Quality Trade-off}: Given a Ratio of 0.75, we compared high-step (30|40) and low-step (9|12) strategies. Although 30|40 yielded slightly higher conceptual improvement, its Mask IoU dropped significantly (0.93), increasing the risk of geometric drift. In contrast, \textbf{Steps: 9|12} maintained extremely high geometric stability (\textbf{IoU $\sim$0.96}) while still achieving competitive conceptual enhancement, with a \textbf{70\%} reduction in computational cost.
\end{enumerate}

Based on the above analysis, we adopt the configuration \textbf{(Steps: 9|12, Cfg=4)} in all subsequent 3D experiments.

\subsection{Main Results}

We evaluated the performance of OREO and its baselines on the test set containing 100 concept design prompts.

\subsubsection{Quantitative Evaluation.}
Table [Table 1] shows the scores of each method on CLIP Similarity and DINO Similarity.
\begin{itemize}
    \item \textbf{Conceptual Alignment}: RED achieved a significant lead in CLIP Similarity, improving by about 15\% compared to the original Trellis. This indicates that our method successfully transferred the 2D editor's understanding of complex concepts to the 3D generator.
    \item \textbf{User Preference}: User Study results show that over 85\% of users prefer the assets generated by RED in terms of geometric plausibility and texture details, especially when dealing with non-standard structures of creatures and vehicles.
\end{itemize}

\subsubsection{Qualitative Evaluation.}
Figure [Figure Y] shows a visual comparison between RED and the baseline models.
\begin{itemize}
    \item \textbf{Geometric Repair}: In the case of "Cyberpunk style mechanical prosthetic limbs", the original Trellis often generated blurred structures, while RED successfully restored clear joint and pipeline details.
    \item \textbf{Texture Enhancement}: For "Magic book with glowing runes", the textures generated by RED were not only clearer but also had lighting effects more consistent with the prompt description, eliminating the oversaturation problem common in SDS.
    \item \textbf{Multi-view Consistency}: Although FlowEdit guides on a single view, thanks to the inherent consistency of the 3D generator, the assets generated by RED maintained structural coherence across all views, without obvious Janus problems.
\end{itemize}

\subsection{Ablation Study and Analysis}

To deeply understand the contribution of each component of RED, we conducted a series of ablation experiments.

\subsubsection{Analyzing Loss Components.}
We verified the necessity of each part of the Contrastive Distillation Loss (CDL).
\begin{itemize}
    \item \textbf{Replace CDL with MSE}: We attempted to directly minimize the mean squared error ($\| x^{src} - x^{tgt} \|^2$) between the rendered image and the final output of FlowEdit. Results showed that although this strategy could quickly close the conceptual distance, it easily led to geometric distortion (e.g., uneven surfaces). This is because MSE forces the model to match the editing results at the pixel level, ignoring inevitable minor geometric deviations during the 2D editing process. In contrast, CDL utilizes relative gradient directions, providing more robust conceptual guidance.
    \item \textbf{Remove Repeller Term ($\| x^{src} - \hat{x}^{-}_{t \to 0} \|^2$)}: Retaining only the attractor term led to the model overfitting the editing target, ignoring the suppression of the unconditional distribution, resulting in generated textures that were too smooth and lacked detail.
    \item \textbf{Remove Trajectory Regularization ($\mathcal{L}_{reg}$)}: This is a key stability term. Experiments showed that without regularization, the model was prone to geometric collapse (e.g., broken surfaces or excess floating objects) in the later stages of training, proving the importance of maintaining 3D priors during the distillation process.
\end{itemize}

\subsubsection{Sampling Strategy Analysis.}
To balance training efficiency and performance, we adopted Multi-step Time Sampling (MTS Sampling).
\begin{itemize}
    \item \textbf{Fixed Timestep}: If optimization is performed only at a fixed noise level (e.g., $t=500$), the model struggles to handle diverse input distributions, resulting in insufficient texture detail after convergence.
    \item \textbf{MTS Sampling (Ours)}: To address the overfitting problem caused by fixed timesteps, we adopted a \textbf{Stratified Random Sampling} strategy. Unlike solving ODEs on a fixed discrete grid, we divide the entire time axis into continuous sub-intervals and uniformly sample a time step from each sub-interval in each iteration. This strategy ensures that the model covers continuous noise levels while maintaining the correctness of the Global Flow Trajectory, significantly enhancing robustness to minor time shifts.
\end{itemize}

\section{Conclusion}
This paper proposes \textbf{RED (Reinforced Editing Distillation)}, a general post-training framework for 3D concept design scenarios. Addressing the generalization bottleneck of existing 3D generative models when handling highly stylized and non-standard geometric inputs, we innovatively introduce an enhanced editing mechanism based on FlowEdit. By constructing a \textbf{Contrastive Distillation Loss} combined with \textbf{trajectory regularization}, RED successfully distills the rich conceptual priors contained in 2D foundation models into 3D generators. Experimental results show that our method significantly outperforms supervised baselines in conceptual consistency, geometric fidelity, and texture detail, providing an efficient new path for solving the data scarcity problem in 3D generation.

\textbf{Future Work.} Although RED performs well, there is still room for further exploration. First, the current FlowEdit process has high inference costs; future work could explore more efficient distillation strategies (e.g., one-step editing). Second, we will attempt to extend RED to more complex scene generation tasks, utilizing panoramic editing models to optimize large-scale 3D environments. Finally, combining with Large Multimodal Models (LMM) for more fine-grained interactive editing is also an exciting direction.
