\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version

\usepackage[review,year=2026,ID=153]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}


\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title
\title{OREO: Generalizing 3D Native Generators with On-Policy Rendering-Editing Optimization} 

% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
\titlerunning{Abbreviated paper title}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\author{First Author\inst{1}\orcidlink{0000-1111-2222-3333} \and
Second Author\inst{2,3}\orcidlink{1111-2222-3333-4444} \and
Third Author\inst{3}\orcidlink{2222--3333-4444-5555}}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{F.~Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list. 
\institute{Princeton University, Princeton NJ 08544, USA \and
Springer Heidelberg, Tiergartenstr.~17, 69121 Heidelberg, Germany
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}

\maketitle


\begin{abstract}
  While 3D native generators have shown impressive results, they often produce semantically misaligned outputs in imaginative or stylized scenarios, resulting in geometric distortion or textual artifacts.
  To overcome this limitation, we propose \textbf{OREO}, which leverages pre-trained 2D diffusion priors to post-train 3D generators through an on-policy \textit{Render-Edit-Optimize} loop.
  At the core of OREO, the 2D prior is integrated through our \textbf{Reinforced Editing Distillation (RED)} algorithm. 
  RED employs an enhanced FlowEdit strategy to generate corrective edits that reinforce target concepts while preserving the original viewpoint.
  To optimize the 3D generator, we leverage a contrastive loss for stable gradient updates, which are then backpropagated to every intermediate step of the trajectory. 
  Furthermore, we introduce a 3D native regularization that allows structural constraints from each step to influence all previous stages, effectively preventing geometric degradation.
  Extensive experiments on mainstream baselines, including Trellis and Trellis2, demonstrate that OREO significantly enhances their generalization capabilities on complex creative inputs.
  \keywords{3D Generation \and On-Policy Distillation \and Image Editing}
\end{abstract}

% NOTE: Terminology Distinction
% - Semantic: Refers to high-level understanding, knowledge, and alignment (e.g., "semantic priors", "semantic misalignment").
% - Concept: Refers to specific objects, attributes, or content being generated/injected (e.g., "inject target concepts", "complex concepts").
% - Post-training Domain: Targets OOD domains (stylized/imaginative) uncovered by pre-training, bridging the gap via cross-modal transfer.

\section{Introduction}
\label{sec:intro}

Recently, 3D native generators such as Trellis and Hunyuan3D series have emerged as the mainstream approach in automating high-quality 3D content creation due to their superior capabilities in producing high-fidelity geometry and detailed textures.
However, the performance of these pre-trained models is fundamentally constrained by the available 3D datasets, which remain at the million-scale due to the prohibitive costs of asset construction. 
Consequently, these datasets are limited in diversity, posing a significant challenge for creative applications like 3D concept design. 
In such scenarios, the input concepts are often highly stylized or imaginative, significantly deviating from the training distribution and resulting in semantic misalignment, which manifests as geometric distortion or textual artifacts.

To address this challenge, we leverage the powerful priors of pre-trained 2D diffusion models, which benefit from training on billion-scale image datasets. 
Such extensive training equips them with robust semantic understanding and superior capabilities in generating imaginative structures and intricate details, far surpassing the scope of existing million-scale 3D datasets. 
To effectively distill these rich 2D priors into 3D generators, we propose the \textbf{On-Policy Rendering Editing Optimization (OREO)} framework. 
The core idea is to establish an on-policy optimization mechanism: instead of learning from static datasets, the generator receives immediate feedback on its own artifacts from a 2D teacher. 
Specifically, OREO implements a \textit{Render-Edit-Optimize} loop. 
In this process, the model renders views from its current rollout, receives corrective edits from the 2D teacher, and uses these dynamic targets for model updates.

At the core of this loop is the \textbf{Reinforced Editing Distillation (RED)} algorithm, which generates corrective guidance through the sequential steps of \textit{view editing} and \textit{gradient distillation}. 
Specifically, for view editing, we employ an improved FlowEdit strategy to generate corrective edits. 
This process reinforces target concepts in a view-preserving manner, yielding view-consistent refined targets. 
Subsequently, to stably distill these signals, we propose a \textbf{contrastive loss} that leverages these targets to construct positive and negative sample pairs, achieving stable gradient updates. 
To optimize the 3D generator, OREO backpropagates the editing gradients calculated at the terminal state to every intermediate step of the entire generation trajectory.  
Furthermore, along with the editing backpropagation, we incorporate a 3D native regularization that propagates structural constraints back to early generation steps, effectively preventing geometric degradation caused by aggressive view-space editing. 
Extensive experiments on mainstream baselines, including Trellis and Trellis2, demonstrate that OREO significantly enhances their generalization capabilities on complex creative inputs.

In summary, our contributions are as follows:
\begin{enumerate}
    \item We introduce OREO, a novel post-training paradigm for 3D native generators via feedback from pre-trained 2D diffusion priors. 
    By establishing an on-policy Render-Edit-Optimize loop, it effectively compensates for 3D data scarcity, enabling robust generalization to highly stylized and imaginative concepts.
    \item To construct feedback from 2D priors, we propose the Reinforced Editing Distillation (RED) algorithm. 
    It generates corrective guidance by editing rendered views with an improved FlowEdit strategy and distilling these corrections into stable gradients via a contrastive loss.
    \item We implement an optimization mechanism that propagates editing guidance to all intermediate steps. 
    This mechanism further enables 3D native regularization, allowing structural constraints from each step to influence all previous stages, thereby preventing structural degradation.
    \item We validate the effectiveness of OREO on mainstream baselines, including Trellis and Trellis2. Results show that our method significantly outperforms supervised baselines in both conceptual alignment and geometric fidelity on complex creative inputs
\end{enumerate}

\section{Method}
\label{sec:method}

\subsection{Preliminaries}
\label{sec:preliminaries}

\subsubsection{Problem Formulation.}
We consider a conditional 3D generative model based on Flow Matching, parameterized by a velocity field $v_\theta$ and a decoder $\mathcal{D}$. 
Following the mainstream Image-to-3D paradigm, the model takes a reference image $x^{ref}$ as conditioning input. 
The generation process is modeled as a conditional ODE integration over a time horizon $t \in [0, 1]$, mapping a prior noise distribution $z_1 \sim \mathcal{N}(0, I)$ to the latent data distribution $z_0$:
\begin{equation}
dz_t = v_\theta(z_t; t, x^{ref}) dt
\end{equation}
where $v_\theta$ is the conditional velocity field. We denote the entire ODE integration process as $G_\theta$, such that $z_0 = G_\theta(z_1, x^{ref})$.
For cascaded models (e.g., Trellis), we omit intermediate representations from our formulation for clarity.
Subsequently, the decoder $\mathcal{D}$ maps the latent code $z_0$ to an explicit 3D asset $A = \mathcal{D}(z_0)$ (e.g., Gaussian Splats). 
This asset $A$ can be projected into 2D images $x = \mathcal{P}(A, \pi)$ via a differentiable renderer $\mathcal{P}$ under arbitrary camera poses $\pi$. 
Our objective is to optimize the parameters $\theta$ on a dataset $\mathcal{S}$ containing highly imaginative and stylized images without corresponding ground-truth 3D assets. 
This is challenging due to the misalignment caused by the domain gap between $\mathcal{S}$ and the pre-training data, compounded by the lack of 3D ground truth. 
Therefore, we seek to obtain $\theta^*$ by minimizing the divergence $\mathcal{L}$ between the generated distribution and the target distribution conditioned on $x^{ref}$:
\begin{equation}
\theta^* = \arg\min_\theta \mathbb{E}_{x^{ref} \sim \mathcal{S}, z_1 \sim \mathcal{N}(0,I), \pi, x=\mathcal{P}(\mathcal{D}(G_\theta(z_1, x^{ref})), \pi)} [\mathcal{L}(x, x^{ref})]
\end{equation}
In our framework, we instantiate $\mathcal{L}$ by treating the rendered views $x$ as source images $x^{src}$ and constructing corrective targets $x^{tgt}$ via a 2D editor guided by $x^{ref}$, as detailed in the following sections.

\subsubsection{2D Priors for Structure-Preserving Editing.}
We leverage a pre-trained image editor (parameterized by $v_\phi$) to provide semantic feedback, transforming a source image $x^{src}$ into a target image $x^{tgt}$ aligned with a text instruction $y$, as is standard in mainstream foundation models.
Crucially, for 3D consistency, $\mathcal{E}$ must preserve the geometric structure of $x^{src}$. To achieve this, we implement $\mathcal{E}$ using the FlowEdit algorithm, which leverages the deterministic nature of Probability Flow ODEs. Specifically, it couples the source and target generation trajectories by applying the source flow difference to the target flow:
\begin{equation}
z_{t-\Delta t}^{tgt} = z_t^{tgt} + (v_\phi(z_t^{tgt}, t, c_{tgt}) - v_\phi(z_t^{src}, t, c_{src})) \Delta t
\end{equation}
This mechanism ensures that the editing process modifies semantics $c_{tgt}$ while maintaining the underlying layout of $x^{src}$, making it an ideal candidate for our 2D teacher. We denote this entire editing process as $\mathcal{E}_\phi$, such that $x^{tgt} = \mathcal{E}_\phi(x^{src}, y)$.

\subsection{Overview of OREO Framework}
\label{sec:overview}
To bridge the gap between limited 3D data and open-world creativity, we propose OREO, an On-Policy Rendering-Editing-Optimization framework. As illustrated in Figure~\ref{fig:overview}, OREO establishes a closed-loop feedback mechanism:
\begin{itemize}
    \item \textbf{Render}: The 3D generator produces a 3D asset from a given prompt, which is rendered into multiple 2D views.
    \item \textbf{Edit}: A 2D teacher model edits these views to correct semantic misalignments while preserving the original viewpoint, generating "pseudo-ground truth" targets.
    \item \textbf{Optimize}: The difference between the rendered views and the edited targets is distilled into gradients via a contrastive loss, which are then backpropagated to update the 3D generator.
\end{itemize}
Unlike standard Score Distillation Sampling (SDS) which relies on noisy gradients from a frozen teacher, OREO explicitly constructs high-quality target images, enabling more stable and interpretable optimization.

\subsection{Reinforced Editing Distillation (RED)}
\label{sec:red}
The core of OREO is the Reinforced Editing Distillation (RED) algorithm, which generates robust supervision signals from the 2D teacher.

\subsubsection{View-Consistent Corrective Editing.}
We employ an improved \textbf{FlowEdit} strategy to generate corrective edits. Standard image editing often alters the viewpoint or geometry of the object, leading to inconsistencies when used as 3D supervision. To address this, our enhanced FlowEdit introduces:
\begin{enumerate}
    \item \textbf{Negative Guidance}: We use a negative guidance scale to preserve background and geometric features that should not be modified, ensuring the edit is local and semantically focused.
    \item \textbf{Dynamic Noise Correction}: We adjust the noise level $\epsilon_t$ in real-time based on the source flow field gradient, ensuring the editing trajectory remains anchored to the manifold of the rendered image.
\end{enumerate}
This process yields a view-consistent target image $x_{edit}$ that reinforces the target concept without degrading the original 3D structure.

\subsubsection{Contrastive Gradient Distillation.}
To distill the editing signals into stable gradients, we propose a \textbf{Contrastive Distillation Loss}. Instead of a simple MSE loss, which is sensitive to pixel-level misalignment, we construct positive and negative pairs:
\begin{equation}
\mathcal{L}_{\text{RED}} = \sum_{t} \omega(t) \left[ \| x^{src} - \hat{x}^{+}_{t \to 0} \|^2 - \| x^{src} - \hat{x}^{-}_{t \to 0} \|^2 \right]
\end{equation}
where $\hat{x}^{+}_{t \to 0}$ is the conditional prediction (positive sample) from the editor, and $\hat{x}^{-}_{t \to 0}$ is the unconditional prediction (negative sample). This contrastive formulation pushes the generation towards the edited target while pulling it away from the generic unconditional distribution, resulting in sharper and more semantically aligned updates.

\subsection{Optimization via Trajectory Propagation}
\label{sec:optimization}

\subsubsection{Backpropagation to Intermediate Steps.}
OREO optimizes the 3D generator by backpropagating gradients from the final rendered image to the entire generation trajectory. Since the generation process is a discretized ODE solve:
\begin{equation}
z_{t_{i-1}} = z_{t_i} - (t_i - t_{i-1}) \cdot v_\theta(z_{t_i}, t_i)
\end{equation}
We retain the computation graph at every step, allowing the gradients from the terminal loss $\mathcal{L}_{\text{RED}}$ to influence the velocity field predictions $v_\theta$ at all intermediate time steps $t_i$.

\subsubsection{3D Native Regularization.}
Aggressive 2D editing can sometimes disrupt the underlying 3D geometry. To prevent geometric degradation, we introduce a 3D native regularization term:
\begin{equation}
\mathcal{L}_{reg} = \mathbb{E}_{t} \left[ \| \hat{z}_{0,\theta}(z_t, t) - \hat{z}_{0,frozen}(z_t, t) \|^2 \right]
\end{equation}
This term constrains the predicted $z_0$ at each step to stay close to the prediction of the frozen pre-trained model, ensuring that the optimization respects the structural priors of the original 3D generator. The final objective is $\mathcal{L}_{total} = \mathcal{L}_{RED} + \lambda \mathcal{L}_{reg}$.


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Dataset.}
To validate the effectiveness of OREO in \textbf{3D concept design} scenarios, we constructed a specialized \textbf{Conceptual Design Dataset}.
\begin{itemize}
    \item \textbf{Training Set}: Contains approximately 2,000 high-quality concept design images collected from the internet, covering sci-fi vehicles, fantasy creatures, stylized characters, and futuristic architecture. These images typically feature exaggerated proportions, unique textures, and non-realistic geometric structures, posing extremely high demands on the generalization capability of 3D generators.
    \item \textbf{Test Set}: Contains 100 unseen, highly imaginative design sketches, used to evaluate the model's ability to translate abstract ideas into 3D entities.
\end{itemize}

\subsubsection{Baselines.}
We compare OREO with the following baseline:
\begin{itemize}
    \item \textbf{Trellis (Zero-shot)}: The original pre-trained Trellis model, serving as the supervised learning benchmark.
\end{itemize}

\subsubsection{Metrics.}
We employ a combination of quantitative and qualitative evaluations:
\begin{itemize}
    \item \textbf{CLIP Similarity}: Measures the conceptual consistency between generated views and the input prompt.
    \item \textbf{DINO Similarity}: Evaluates the visual feature similarity between generated results and reference images.
    \item \textbf{User Study}: Human evaluators are invited to score the geometric quality and texture details of the generated assets.
\end{itemize}

\subsubsection{Implementation Details.}
Our experiments are implemented based on the PyTorch framework.
\begin{itemize}
    \item \textbf{Model Architecture}: We use the pre-trained \textbf{Trellis-Image-Large} as the 3D generator backbone and \textbf{Qwen-Image-Edit} as the 2D editing teacher model. The renderer is Gaussian Splatting, with a rendering resolution of $1024 \times 1024$.
    \item \textbf{Training Settings}: We use the \textbf{SGD optimizer} with a learning rate of $5 \times 10^{-3}$. Training is conducted on a single NVIDIA A800 GPU with a batch size of 1 and gradient accumulation steps of 4. To save memory, we use \textbf{BF16 mixed precision} training. The total number of training epochs is 500.
    \item \textbf{RED Parameters}: The noise mode uses the "Aligned" strategy. Based on preliminary analysis (see Section 4.2), we fix the configuration to \textbf{(Steps: 9|12, Cfg=4)}, meaning editing starts at a noise level of $t=0.75$ with a total of 12 inference steps.
    \item \textbf{Loss Weights}: In all experiments, we only use the \textbf{Contrastive Distillation Loss (CDL)} with a weight of 1.0, without enabling additional MSE or regularization losses.
\end{itemize}

\subsection{Preliminary Analysis: Validating the Editing Teacher}

Before applying FlowEdit to the 3D optimization loop, we first evaluated its competence as a "teacher model" on an independent 2D dataset and determined the optimal editing parameters. The upper bound of 3D optimization depends on the quality of the 2D Pseudo-GT: we need to ensure that the $\Delta x$ produced by the editing process is primarily semantic correction rather than geometric destruction.

We selected 50 rendered images from the test set, executed FlowEdit under different parameter configurations, and calculated CLIP Similarity (conceptual alignment), DINO Similarity (structural consistency), and Silhouette IoU (contour overlap). Additionally, we compared FlowEdit with other mainstream image editing methods. Table \ref{tab:flowedit_ablation} shows the detailed quantitative analysis results.

\begin{table}[tb]
  \caption{Sensitivity analysis of FlowEdit parameters. We report the change in metrics (Diff) and the final Mask IoU under different configurations. The gray background indicates the selected final configuration.}
  \label{tab:flowedit_ablation}
  \centering
  \begin{tabular}{@{}llccc@{}}
    \toprule
    Experiment & Configuration & CLIP Sim Diff$\uparrow$ & DINO Sim Diff$\uparrow$ & Mask IoU$\uparrow$ \\
    \midrule
    \textbf{A. CFG Scale} & Steps: 20|40, Cfg=2 & +0.0029 & +0.0028 & 0.9887 \\
    \textit{(Fixed Steps)} & \textbf{Steps: 20|40, Cfg=4} & \textbf{+0.0034} & \textbf{+0.0089} & \textbf{0.9786} \\
    & Steps: 20|40, Cfg=12 & -0.0098 & +0.0051 & 0.9631 \\
    \midrule
    \textbf{B. Ratio (3:4 vs 2:4)} & Steps: 20|40 (Ratio 0.50) & +0.0034 & +0.0089 & 0.9786 \\
    \textit{(Fixed Cfg=4)} & \textbf{Steps: 30|40 (Ratio 0.75)} & \textbf{+0.0228} & \textbf{+0.0226} & \textbf{0.9319} \\
    \midrule
    \textbf{C. Efficiency} & Steps: 30|40 (Total 40) & +0.0228 & +0.0226 & 0.9319 \\
    \textit{(Fixed Ratio 0.75)} & \textbf{Steps: 9|12 (Total 12)} & \textbf{+0.0132} & \textbf{+0.0156} & \textbf{0.9595} \\
    \bottomrule
  \end{tabular}
\end{table}

\textbf{Analysis and Conclusion}:
\begin{enumerate}
    \item \textbf{Choice of Guidance Scale}: Experiments show that excessively high CFG (e.g., 12) severely damages image quality, leading to drops in both CLIP and IoU. \textbf{Cfg=4} demonstrated the most robust performance, effectively enhancing conceptual features while maintaining high geometric consistency (IoU > 0.97).
    \item \textbf{Critical Role of Timestep Ratio}: We found that the starting point of the editing interval is more critical than the total number of steps. Comparing \textbf{Steps: 20|40 (Ratio 0.5)} and \textbf{Steps: 30|40 (Ratio 0.75)}, the latter brought nearly a \textbf{7x} improvement in CLIP (+0.0228 vs +0.0034). This indicates that at low noise levels like $t=0.5$, the image structure is solidified, and the model lacks sufficient \textbf{plasticity} to inject new concepts. Therefore, we fix the \textbf{Ratio to 0.75} to ensure sufficient editing freedom.
    \item \textbf{Efficiency-Quality Trade-off}: Given a Ratio of 0.75, we compared high-step (30|40) and low-step (9|12) strategies. Although 30|40 yielded slightly higher conceptual improvement, its Mask IoU dropped significantly (0.93), increasing the risk of geometric drift. In contrast, \textbf{Steps: 9|12} maintained extremely high geometric stability (\textbf{IoU $\sim$0.96}) while still achieving competitive conceptual enhancement, with a \textbf{70\%} reduction in computational cost.
\end{enumerate}

Based on the above analysis, we adopt the configuration \textbf{(Steps: 9|12, Cfg=4)} in all subsequent 3D experiments.

\subsection{Main Results}

We evaluated the performance of OREO and its baselines on the test set containing 100 concept design prompts.

\subsubsection{Quantitative Evaluation.}
Table [Table 1] shows the scores of each method on CLIP Similarity and DINO Similarity.
\begin{itemize}
    \item \textbf{Conceptual Alignment}: RED achieved a significant lead in CLIP Similarity, improving by about 15\% compared to the original Trellis. This indicates that our method successfully transferred the 2D editor's understanding of complex concepts to the 3D generator.
    \item \textbf{User Preference}: User Study results show that over 85\% of users prefer the assets generated by RED in terms of geometric plausibility and texture details, especially when dealing with non-standard structures of creatures and vehicles.
\end{itemize}

\subsubsection{Qualitative Evaluation.}
Figure [Figure Y] shows a visual comparison between RED and the baseline models.
\begin{itemize}
    \item \textbf{Geometric Repair}: In the case of "Cyberpunk style mechanical prosthetic limbs", the original Trellis often generated blurred structures, while RED successfully restored clear joint and pipeline details.
    \item \textbf{Texture Enhancement}: For "Magic book with glowing runes", the textures generated by RED were not only clearer but also had lighting effects more consistent with the prompt description, eliminating the oversaturation problem common in SDS.
    \item \textbf{Multi-view Consistency}: Although FlowEdit guides on a single view, thanks to the inherent consistency of the 3D generator, the assets generated by RED maintained structural coherence across all views, without obvious Janus problems.
\end{itemize}

\subsection{Ablation Study and Analysis}

To deeply understand the contribution of each component of RED, we conducted a series of ablation experiments.

\subsubsection{Analyzing Loss Components.}
We verified the necessity of each part of the Contrastive Distillation Loss (CDL).
\begin{itemize}
    \item \textbf{Replace CDL with MSE}: We attempted to directly minimize the mean squared error ($\| x^{src} - x^{tgt} \|^2$) between the rendered image and the final output of FlowEdit. Results showed that although this strategy could quickly close the conceptual distance, it easily led to geometric distortion (e.g., uneven surfaces). This is because MSE forces the model to match the editing results at the pixel level, ignoring inevitable minor geometric deviations during the 2D editing process. In contrast, CDL utilizes relative gradient directions, providing more robust conceptual guidance.
    \item \textbf{Remove Repeller Term ($\| x^{src} - \hat{x}^{-}_{t \to 0} \|^2$)}: Retaining only the attractor term led to the model overfitting the editing target, ignoring the suppression of the unconditional distribution, resulting in generated textures that were too smooth and lacked detail.
    \item \textbf{Remove Trajectory Regularization ($\mathcal{L}_{reg}$)}: This is a key stability term. Experiments showed that without regularization, the model was prone to geometric collapse (e.g., broken surfaces or excess floating objects) in the later stages of training, proving the importance of maintaining 3D priors during the distillation process.
\end{itemize}

\subsubsection{Sampling Strategy Analysis.}
To balance training efficiency and performance, we adopted Multi-step Time Sampling (MTS Sampling).
\begin{itemize}
    \item \textbf{Fixed Timestep}: If optimization is performed only at a fixed noise level (e.g., $t=500$), the model struggles to handle diverse input distributions, resulting in insufficient texture detail after convergence.
    \item \textbf{MTS Sampling (Ours)}: To address the overfitting problem caused by fixed timesteps, we adopted a \textbf{Stratified Random Sampling} strategy. Unlike solving ODEs on a fixed discrete grid, we divide the entire time axis into continuous sub-intervals and uniformly sample a time step from each sub-interval in each iteration. This strategy ensures that the model covers continuous noise levels while maintaining the correctness of the Global Flow Trajectory, significantly enhancing robustness to minor time shifts.
\end{itemize}

\section{Conclusion}
This paper proposes \textbf{RED (Reinforced Editing Distillation)}, a general post-training framework for 3D concept design scenarios. Addressing the generalization bottleneck of existing 3D generative models when handling highly stylized and non-standard geometric inputs, we innovatively introduce an enhanced editing mechanism based on FlowEdit. By constructing a \textbf{Contrastive Distillation Loss} combined with \textbf{trajectory regularization}, RED successfully distills the rich conceptual priors contained in 2D foundation models into 3D generators. Experimental results show that our method significantly outperforms supervised baselines in conceptual consistency, geometric fidelity, and texture detail, providing an efficient new path for solving the data scarcity problem in 3D generation.

\textbf{Future Work.} Although RED performs well, there is still room for further exploration. First, the current FlowEdit process has high inference costs; future work could explore more efficient distillation strategies (e.g., one-step editing). Second, we will attempt to extend RED to more complex scene generation tasks, utilizing panoramic editing models to optimize large-scale 3D environments. Finally, combining with Large Multimodal Models (LMM) for more fine-grained interactive editing is also an exciting direction.

\end{document}
